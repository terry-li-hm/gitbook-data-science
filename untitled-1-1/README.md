# Evaluation

* [marcotcr/lime: Lime: Explaining the predictions of any machine learning classifier](https://github.com/marcotcr/lime)
* [slundberg/shap: A unified approach to explain the output of any machine learning model](https://github.com/slundberg/shap)
* [Aequitas - The Bias Report](http://aequitas.dssg.io/)
* [How \(and why\) to create a good validation set · fast.ai](http://www.fast.ai/2017/11/13/validation-sets/)
* [How to unit test machine learning code. – Chase Roberts – Medium](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765)
* [lk-geimfari/mimesis: Mimesis is a fast and easy to use library for Python, which helps generate synthetic data for a variety of purposes in a variety of languages.](https://github.com/lk-geimfari/mimesis)
* [A Complete Machine Learning Walk-Through in Python: Part Two](https://towardsdatascience.com/a-complete-machine-learning-project-walk-through-in-python-part-two-300f1f8147e2)
* [TeamHG-Memex/eli5: A library for debugging/inspecting machine learning classifiers and explaining their predictions](https://github.com/TeamHG-Memex/eli5)
* [Evaluating Machine Learning Models](https://www.oreilly.com/data/free/files/evaluating-machine-learning-models.pdf)
* [Use of swap set to measure impact of model changes](http://www.experian.com/blogs/insights/2018/01/swap-set-measure-impact-model-changes/)
* [How to Calculate McNemar's Test to Compare Two Machine Learning Classifiers](https://machinelearningmastery.com/mcnemars-test-for-machine-learning/)



### Cross-validation

* [Example of CV](https://www.kaggle.com/rspadim/off-example-of-cv/)
* [Cross- Validation Code Visualization: Kind of Fun](https://medium.com/towards-data-science/cross-validation-code-visualization-kind-of-fun-b9741baea1f8)



