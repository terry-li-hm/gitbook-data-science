# Evaluation

* [marcotcr/lime: Lime: Explaining the predictions of any machine learning classifier](https://github.com/marcotcr/lime)
* [slundberg/shap: A unified approach to explain the output of any machine learning model](https://github.com/slundberg/shap)
* [Aequitas - The Bias Report](http://aequitas.dssg.io/)
* [How \(and why\) to create a good validation set · fast.ai](http://www.fast.ai/2017/11/13/validation-sets/)
* [How to unit test machine learning code. – Chase Roberts – Medium](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765)
* [lk-geimfari/mimesis: Mimesis is a fast and easy to use library for Python, which helps generate synthetic data for a variety of purposes in a variety of languages.](https://github.com/lk-geimfari/mimesis)
* re[A Complete Machine Learning Walk-Through in Python: Part Two](https://towardsdatascience.com/a-complete-machine-learning-project-walk-through-in-python-part-two-300f1f8147e2)
* [TeamHG-Memex/eli5: A library for debugging/inspecting machine learning classifiers and explaining their predictions](https://github.com/TeamHG-Memex/eli5)
* [Evaluating Machine Learning Models](https://www.oreilly.com/data/free/files/evaluating-machine-learning-models.pdf)
* [Use of swap set to measure impact of model changes](http://www.experian.com/blogs/insights/2018/01/swap-set-measure-impact-model-changes/)
* [How to Calculate McNemar's Test to Compare Two Machine Learning Classifiers](https://machinelearningmastery.com/mcnemars-test-for-machine-learning/)
* [How to Evaluate your Machine Learning Model – Coinmonks – Medium](https://medium.com/coinmonks/debugging-a-learning-algorithm-ef7c16936864)
* [Testing machine learning models with testthat · Gordon Shotwell](https://blog.shotwell.ca/2018/05/01/testing-machine-learning-models-with-testthat/)
* [sepandhaghighi/pycm: Multi-class confusion matrix library in Python.](https://github.com/sepandhaghighi/pycm)
* \*\*\*\*[**Testing and Debugging in Machine Learning \(Google Developers\)**](https://developers.google.com/machine-learning/testing-debugging/)\*\*\*\*





### Cross-validation

* [Example of CV](https://www.kaggle.com/rspadim/off-example-of-cv/)
* [Cross- Validation Code Visualization: Kind of Fun](https://medium.com/towards-data-science/cross-validation-code-visualization-kind-of-fun-b9741baea1f8)



## Baseline

To judge whether our model is any good, it'd be helpful to have a baseline and see if the model performs better than the baseline. A commonly used baseline is to predict the target variable as the mean of training set.



## 



