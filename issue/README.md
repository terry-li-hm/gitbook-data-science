# Issue

* [**Toward ethical, transparent and fair AI/ML: a critical reading list**](https://medium.com/@eirinimalliaraki/toward-ethical-transparent-and-fair-ai-ml-a-critical-reading-list-d950e70a70ea)
* [Towards fairness in ML with adversarial networks](https://blog.godatadriven.com/fairness-in-ml)
* [natural language processing blog: Many opportunities for discrimination in deploying machine learning systems](https://nlpers.blogspot.com/2018/06/many-opportunities-for-discimination-in.html)
* [Explain yourself, machine. Producing simple text descriptions for AI interpretability. – Luke Oakden-Rayner](https://lukeoakdenrayner.wordpress.com/2018/06/05/explain-yourself-machine-producing-simple-text-descriptions-for-ai-interpretability/)
* [Managing risk in machine learning models - O'Reilly Media](https://www.oreilly.com/ideas/managing-risk-in-machine-learning-models)
* [We need to build machine learning tools to augment machine learning engineers - O'Reilly Media](https://www.oreilly.com/ideas/we-need-to-build-machine-learning-tools-to-augment-machine-learning-engineers)
* [Scientific debt – Variance Explained](http://varianceexplained.org/r/scientific-debt/)
* [Data Ethics Framework - GOV.UK](https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework)
* [**Attacks against machine learning — an overview**](https://elie.net/blog/ai/attacks-against-machine-learning-an-overview)
* [Challenges faced while training an AI to combat abuse](https://elie.net/blog/ai/challenges-faced-while-training-an-ai-to-combat-abuse)
* [natural language processing blog: Many opportunities for discrimination in deploying machine learning systems](https://nlpers.blogspot.com/2018/06/many-opportunities-for-discimination-in.html)
* [\[1807.01697\] Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations](https://arxiv.org/abs/1807.01697)
* [California Shopping Centers Are Spying for an ICE Contractor \| Electronic Frontier Foundation](https://www.eff.org/deeplinks/2018/07/california-shopping-centers-are-spying-ice-contractor)
* [The Best Research Papers from ICML 2018 - A Must-Read for Data Scientists](https://www.analyticsvidhya.com/blog/2018/06/best-research-papers-icml-2018/)
* [Hedge funds worry about the legal risks of using “alternative” data - Wary scouts](https://www.economist.com/news/finance-and-economics/21744851-dubious-data-providers-emerge-established-players-start-offering-analysis)
* [Medical AI Safety: We have a problem. – Luke Oakden-Rayner](https://lukeoakdenrayner.wordpress.com/2018/07/11/medical-ai-safety-we-have-a-problem/)
* [Of oaths and checklists - O'Reilly Media](https://www.oreilly.com/ideas/of-oaths-and-checklists)
* [Regulating AI in the era of big tech](https://thegradient.pub/regulating-ai-in-the-era-of-big-tech/)
* [Doing good data science - O'Reilly Media](https://www.oreilly.com/ideas/doing-good-data-science?mkt_tok=eyJpIjoiTVRFM05XTTROR1k1WW1FNCIsInQiOiJSekx3ZzBNcnFxK0JiVDN1VEZvN05RYnBUU2JRVHh2UCswMW8wUkVBdThDNDZNUmh6ZHo1allsWHV0K09UdUVJbDBKZEhPd29NS01iRndhZEd0ZFBiOXhoRktqQ0hHVTZ2S05DQUpwd0hjVXErclNsWmw2S3diQU94c045dHlidyJ9)
* [AI can be sexist and racist — it’s time to make it fair](https://www.nature.com/articles/d41586-018-05707-8)
* [Why Self-Driving Cars Must Be Programmed to Kill - MIT Technology Review](https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/)
* [Doing good data science - O'Reilly Media](https://www.oreilly.com/ideas/doing-good-data-science?mkt_tok=eyJpIjoiWXpsaE1HWmhPV1U0WkdGayIsInQiOiJxYWlXblZQdVZFWFMzR3VKUFNmT1wvQ3pmWGZibHU2ZHVLRllRdTVQWjdIU1M4TTN0UmF4VjZsNGtRUmVqcElQYlo1TVRGdHNUXC96RGdxN2luUlc2UUFlRDNOM2dJXC9VUTJjSFJpQWR0WGJtTytMWkd4OHNSMUplSjlScHYycUZ2ZyJ9)
* \*\*\*\*[**Inclusive ML \| Google Cloud**](https://cloud.google.com/inclusive-ml/)\*\*\*\*
* [You and AI – Just An Engineer: The Politics of AI - YouTube](https://www.youtube.com/watch?v=HPopJb5aDyA)
* [The five Cs - O'Reilly Media](https://www.oreilly.com/ideas/the-five-cs?mkt_tok=eyJpIjoiWXpsaE1HWmhPV1U0WkdGayIsInQiOiJxYWlXblZQdVZFWFMzR3VKUFNmT1wvQ3pmWGZibHU2ZHVLRllRdTVQWjdIU1M4TTN0UmF4VjZsNGtRUmVqcElQYlo1TVRGdHNUXC96RGdxN2luUlc2UUFlRDNOM2dJXC9VUTJjSFJpQWR0WGJtTytMWkd4OHNSMUplSjlScHYycUZ2ZyJ9)
* [Of oaths and checklists - O'Reilly Media](https://www.oreilly.com/ideas/of-oaths-and-checklists?mkt_tok=eyJpIjoiWXpsaE1HWmhPV1U0WkdGayIsInQiOiJxYWlXblZQdVZFWFMzR3VKUFNmT1wvQ3pmWGZibHU2ZHVLRllRdTVQWjdIU1M4TTN0UmF4VjZsNGtRUmVqcElQYlo1TVRGdHNUXC96RGdxN2luUlc2UUFlRDNOM2dJXC9VUTJjSFJpQWR0WGJtTytMWkd4OHNSMUplSjlScHYycUZ2ZyJ9)
* [Regulating AI in the era of big tech](https://thegradient.pub/regulating-ai-in-the-era-of-big-tech/)
* [Medical AI Safety: We have a problem. – Luke Oakden-Rayner](https://lukeoakdenrayner.wordpress.com/2018/07/11/medical-ai-safety-we-have-a-problem/)
* [Data's day of reckoning - O'Reilly Media](https://www.oreilly.com/ideas/datas-day-of-reckoning)
* [Why Is Google Translate Spitting Out Sinister Religious Prophecies? - Motherboard](https://motherboard.vice.com/en_us/article/j5npeg/why-is-google-translate-spitting-out-sinister-religious-prophecies)
* [AI at Google: our principles](https://blog.google/technology/ai/ai-principles/)
* [Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit](https://www.kdnuggets.com/2018/07/domino-data-science-leaders-summit-highlights.html)
* [Privacy & Security in AI](https://register.gotowebinar.com/recording/viewRecording/6999561782741728001/4127756159774041096/terry.li.hm@gmail.com?registrantKey=8218259372777423884&type=ABSENTEEEMAILRECORDINGLINK)

## Interpretability

* [tensorflow/lucid: A collection of infrastructure and tools for research in neural network interpretability.](https://github.com/tensorflow/lucid)
* [TeamHG-Memex/eli5: A library for debugging/inspecting machine learning classifiers and explaining their predictions](https://github.com/TeamHG-Memex/eli5)
* [Human Interpretable Machine Learning \(Part 1\) — The Need and Importance of Model Interpretation](https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)
* [Testing machine learning interpretability techniques - O'Reilly Media](https://www.oreilly.com/ideas/testing-machine-learning-interpretability-techniques)
* [**lopusz/awesome-interpretable-machine-learning**](https://github.com/lopusz/awesome-interpretable-machine-learning)
* [可微编程：打开深度学习的黑盒子](https://zhuanlan.zhihu.com/p/37863641)
* ["Why Should I Trust You?": Explaining the... & related info \| Mendeley](https://www.mendeley.com/research-papers/i-trust-explaining-predictions-classifier/)
* [Google AI Blog: How Can Neural Network Similarity Help Us Understand Training and Generalization?](https://ai.googleblog.com/2018/06/how-can-neural-network-similarity-help.html?m=1)
* [Inside the Mind of a Neural Network with Interactive Code in Tensorflow \(Histogram, Activation…](https://towardsdatascience.com/inside-the-mind-of-a-neural-network-with-interactive-code-in-tensorflow-histogram-activation-a4dff0963103)
* [Human Interpretable Machine Learning \(Part 1\) — The Need and Importance of Model Interpretation](https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)
* [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/)
* [Understanding model predictions with LIME – Towards Data Science](https://towardsdatascience.com/understanding-model-predictions-with-lime-a582fdff3a3b)
* [Interpreting machine learning models – Towards Data Science](https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f)
* [Using Topological Data Analysis to Understand the Behavior of Convolutional Neural Networks \| Ayasdi](https://www.ayasdi.com/blog/artificial-intelligence/using-topological-data-analysis-understand-behavior-convolutional-neural-networks/)
* [Google AI Blog: How Can Neural Network Similarity Help Us Understand Training and Generalization?](https://ai.googleblog.com/2018/06/how-can-neural-network-similarity-help.html)
* [The problem with ‘explainable AI’ \| TechCrunch](https://techcrunch.com/2018/06/14/the-problem-with-explainable-ai/)
* [\[1806.10758\] Evaluating Feature Importance Estimates](https://arxiv.org/abs/1806.10758)
* [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)
* [tensorflow/tcav](https://github.com/tensorflow/tcav/)
* [\[1602.04938\] "Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938)
* [Ideas on interpreting machine learning - O'Reilly Media](https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning)
* [yosinski/deep-visualization-toolbox: DeepVis Toolbox](https://github.com/yosinski/deep-visualization-toolbox)
* [Understanding Neural Networks by embedding hidden representations \| Rakesh Chada's Blog](https://rakeshchada.github.io/Neural-Embedding-Animation.html)
* [Responsible AI Practices – Google AI](https://ai.google/education/responsible-ai-practices)
* [Interpreting Latent Space and Bias \| Nadja Rhodes](https://iconix.github.io/dl/2018/07/21/bias-and-space)
* [Using Uncertainty to Interpret your Model - Taboola Tech Blog](https://engineering.taboola.com/using-uncertainty-interpret-model/)
* [DALEX and H2O: Machine Learning Model Interpretability And Feature Explanation](http://www.business-science.io/business/2018/07/23/dalex-feature-interpretation.html)
* [Interpreting with Attention and More \| Nadja Rhodes](https://iconix.github.io/dl/2018/07/13/interpret-attn)
* [The Blacker the Box](https://www.locallyoptimistic.com/post/the-blacker-the-box/)
* [slundberg/shap: A unified approach to explain the output of any machine learning model.](https://github.com/slundberg/shap)
* [Ideas on interpreting machine learning - O'Reilly Media](https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning)
* [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)
* [tensorflow/lucid: A collection of infrastructure and tools for research in neural network interpretability.](https://github.com/tensorflow/lucid#differentiable-image-parameterizations-notebooks)
* [A major milestone for the treatment of eye disease \| DeepMind](https://deepmind.com/blog/moorfields-major-milestone/)
  * [Nature Medicine论文展示DeepMind眼疾诊断里程碑：临床专家级、「解决」黑箱问题 \| 机器之心](https://www.jiqizhixin.com/articles/2018-08-14-4)
  * [Google DeepMind AI system diagnoses eye diseases and shows its work - STAT](https://www.statnews.com/2018/08/13/google-deepmind-ai-diagnoses-eye-diseases/)
  * [DeepMind's AI System Identified over 50 Eye Diseases as Accurately as a Doctor](https://www.analyticsvidhya.com/blog/2018/08/deepminds-ai-identified-over-50-eye-diseases-accurately-doctor/)

{% file src="../.gitbook/assets/s41591-018-0107-6.pdf" caption="Clinically applicable deep learning for diagnosis and referral in retinal disease" %}

##  Fairness

* [Fairness in Machine Learning with PyTorch](https://blog.godatadriven.com/fairness-in-pytorch)
* [The Trouble with Bias - NIPS 2017 Keynote - Kate Crawford \#NIPS2017 - YouTube](https://www.youtube.com/watch?v=fMym_BKWQzk)
* [Debugging data: Microsoft researchers look at ways to train AI systems to reflect the real world - The AI Blog](https://blogs.microsoft.com/ai/debugging-data-microsoft-researchers-look-ways-train-ai-systems-reflect-real-world/)
* [What does it really mean for an algorithm to be biased?](https://thegradient.pub/ai-bias/)
* [Delayed Impact of Fair Machine Learning – The Berkeley Artificial Intelligence Research Blog](http://bair.berkeley.edu/blog/2018/05/17/delayed-impact/)
* [natural language processing blog: Many opportunities for discrimination in deploying machine learning systems](https://nlpers.blogspot.com/2018/06/many-opportunities-for-discimination-in.html)
* [Bias detectives: the researchers striving to make algorithms fair](https://www.nature.com/articles/d41586-018-05469-3)
* [Weak and Strong Bias in Machine Learning](https://www.kdnuggets.com/2018/07/weak-strong-bias-machine-learning.html)
* [Fairness in Machine Learning with PyTorch](https://blog.godatadriven.com/fairness-in-pytorch)
* [Machine Learning for fair decisions - Microsoft Research](https://www.microsoft.com/en-us/research/blog/machine-learning-for-fair-decisions/)
* [Cultural Bias in Machine Intelligence \| machinebias.github.io](http://machinebias.org/)
* [Analyzing & Preventing Unconscious Bias in Machine Learning](https://www.infoq.com/presentations/unconscious-bias-machine-learning)
* [Machine Learning and Human Bias - YouTube](https://www.youtube.com/watch?v=59bMh59JQDo)
* [Want Less-Biased Decisions? Use Algorithms.](https://hbr.org/2018/07/want-less-biased-decisions-use-algorithms)
* [What HBR Gets Wrong About Algorithms and Bias · fast.ai](http://www.fast.ai/2018/08/07/hbr-bias-algorithms/)
* [Fairness and Bias in Algorithms — District Data Labs: Data Science Consulting and Training](https://www.districtdatalabs.com/fairness-and-bias-in-algorithms)
* [Machine Bias — ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)



