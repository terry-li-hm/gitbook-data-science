# Issue

* [**Toward ethical, transparent and fair AI/ML: a critical reading list**](https://medium.com/@eirinimalliaraki/toward-ethical-transparent-and-fair-ai-ml-a-critical-reading-list-d950e70a70ea)
* [Towards fairness in ML with adversarial networks](https://blog.godatadriven.com/fairness-in-ml)
* [natural language processing blog: Many opportunities for discrimination in deploying machine learning systems](https://nlpers.blogspot.com/2018/06/many-opportunities-for-discimination-in.html)
* [Explain yourself, machine. Producing simple text descriptions for AI interpretability. – Luke Oakden-Rayner](https://lukeoakdenrayner.wordpress.com/2018/06/05/explain-yourself-machine-producing-simple-text-descriptions-for-ai-interpretability/)
* [Managing risk in machine learning models - O'Reilly Media](https://www.oreilly.com/ideas/managing-risk-in-machine-learning-models)
* [We need to build machine learning tools to augment machine learning engineers - O'Reilly Media](https://www.oreilly.com/ideas/we-need-to-build-machine-learning-tools-to-augment-machine-learning-engineers)
* [Scientific debt – Variance Explained](http://varianceexplained.org/r/scientific-debt/)
* [Data Ethics Framework - GOV.UK](https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework)
* [**Attacks against machine learning — an overview**](https://elie.net/blog/ai/attacks-against-machine-learning-an-overview)
* [Challenges faced while training an AI to combat abuse](https://elie.net/blog/ai/challenges-faced-while-training-an-ai-to-combat-abuse)
* [natural language processing blog: Many opportunities for discrimination in deploying machine learning systems](https://nlpers.blogspot.com/2018/06/many-opportunities-for-discimination-in.html)
* [\[1807.01697\] Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations](https://arxiv.org/abs/1807.01697)
* [California Shopping Centers Are Spying for an ICE Contractor \| Electronic Frontier Foundation](https://www.eff.org/deeplinks/2018/07/california-shopping-centers-are-spying-ice-contractor)
* [The Best Research Papers from ICML 2018 - A Must-Read for Data Scientists](https://www.analyticsvidhya.com/blog/2018/06/best-research-papers-icml-2018/)
* [Hedge funds worry about the legal risks of using “alternative” data - Wary scouts](https://www.economist.com/news/finance-and-economics/21744851-dubious-data-providers-emerge-established-players-start-offering-analysis)
* [Medical AI Safety: We have a problem. – Luke Oakden-Rayner](https://lukeoakdenrayner.wordpress.com/2018/07/11/medical-ai-safety-we-have-a-problem/)
* [Of oaths and checklists - O'Reilly Media](https://www.oreilly.com/ideas/of-oaths-and-checklists)
* [Regulating AI in the era of big tech](https://thegradient.pub/regulating-ai-in-the-era-of-big-tech/)
* [Doing good data science - O'Reilly Media](https://www.oreilly.com/ideas/doing-good-data-science?mkt_tok=eyJpIjoiTVRFM05XTTROR1k1WW1FNCIsInQiOiJSekx3ZzBNcnFxK0JiVDN1VEZvN05RYnBUU2JRVHh2UCswMW8wUkVBdThDNDZNUmh6ZHo1allsWHV0K09UdUVJbDBKZEhPd29NS01iRndhZEd0ZFBiOXhoRktqQ0hHVTZ2S05DQUpwd0hjVXErclNsWmw2S3diQU94c045dHlidyJ9)
* [AI can be sexist and racist — it’s time to make it fair](https://www.nature.com/articles/d41586-018-05707-8)

## Interpretability

* [tensorflow/lucid: A collection of infrastructure and tools for research in neural network interpretability.](https://github.com/tensorflow/lucid)
* [TeamHG-Memex/eli5: A library for debugging/inspecting machine learning classifiers and explaining their predictions](https://github.com/TeamHG-Memex/eli5)
* [Human Interpretable Machine Learning \(Part 1\) — The Need and Importance of Model Interpretation](https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)
* [Testing machine learning interpretability techniques - O'Reilly Media](https://www.oreilly.com/ideas/testing-machine-learning-interpretability-techniques)
* [**lopusz/awesome-interpretable-machine-learning**](https://github.com/lopusz/awesome-interpretable-machine-learning)
* [可微编程：打开深度学习的黑盒子](https://zhuanlan.zhihu.com/p/37863641)
* ["Why Should I Trust You?": Explaining the... & related info \| Mendeley](https://www.mendeley.com/research-papers/i-trust-explaining-predictions-classifier/)
* [Google AI Blog: How Can Neural Network Similarity Help Us Understand Training and Generalization?](https://ai.googleblog.com/2018/06/how-can-neural-network-similarity-help.html?m=1)
* [Inside the Mind of a Neural Network with Interactive Code in Tensorflow \(Histogram, Activation…](https://towardsdatascience.com/inside-the-mind-of-a-neural-network-with-interactive-code-in-tensorflow-histogram-activation-a4dff0963103)
* [Human Interpretable Machine Learning \(Part 1\) — The Need and Importance of Model Interpretation](https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)
* [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/)
* [Understanding model predictions with LIME – Towards Data Science](https://towardsdatascience.com/understanding-model-predictions-with-lime-a582fdff3a3b)
* [Interpreting machine learning models – Towards Data Science](https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f)
* [Using Topological Data Analysis to Understand the Behavior of Convolutional Neural Networks \| Ayasdi](https://www.ayasdi.com/blog/artificial-intelligence/using-topological-data-analysis-understand-behavior-convolutional-neural-networks/)
* [Google AI Blog: How Can Neural Network Similarity Help Us Understand Training and Generalization?](https://ai.googleblog.com/2018/06/how-can-neural-network-similarity-help.html)
* [The problem with ‘explainable AI’ \| TechCrunch](https://techcrunch.com/2018/06/14/the-problem-with-explainable-ai/)
* [\[1806.10758\] Evaluating Feature Importance Estimates](https://arxiv.org/abs/1806.10758)
* [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)

##  Fairness

* [Fairness in Machine Learning with PyTorch](https://blog.godatadriven.com/fairness-in-pytorch)
* [The Trouble with Bias - NIPS 2017 Keynote - Kate Crawford \#NIPS2017 - YouTube](https://www.youtube.com/watch?v=fMym_BKWQzk)
* [Debugging data: Microsoft researchers look at ways to train AI systems to reflect the real world - The AI Blog](https://blogs.microsoft.com/ai/debugging-data-microsoft-researchers-look-ways-train-ai-systems-reflect-real-world/)
* [What does it really mean for an algorithm to be biased?](https://thegradient.pub/ai-bias/)
* [Delayed Impact of Fair Machine Learning – The Berkeley Artificial Intelligence Research Blog](http://bair.berkeley.edu/blog/2018/05/17/delayed-impact/)
* [natural language processing blog: Many opportunities for discrimination in deploying machine learning systems](https://nlpers.blogspot.com/2018/06/many-opportunities-for-discimination-in.html)
* [Bias detectives: the researchers striving to make algorithms fair](https://www.nature.com/articles/d41586-018-05469-3)
* [Weak and Strong Bias in Machine Learning](https://www.kdnuggets.com/2018/07/weak-strong-bias-machine-learning.html)
* [Fairness in Machine Learning with PyTorch](https://blog.godatadriven.com/fairness-in-pytorch)
* [Machine Learning for fair decisions - Microsoft Research](https://www.microsoft.com/en-us/research/blog/machine-learning-for-fair-decisions/)

