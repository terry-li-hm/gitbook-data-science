# Hyperparameter tuning



* [Bayesian Optimization for Hyperparameter Tuning](https://arimo.com/data-science/2016/bayesian-optimization-hyperparameter-tuning/)
* [Hyperparameter Tuning with hyperopt in Python](http://steventhornton.ca/hyperparameter-tuning-with-hyperopt-in-python/)
* [Best Practices for Parameter Tuning on Models](https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/discussion/19083)
* [Avoiding Grid for parameter tuning](https://www.kaggle.com/c/allstate-claims-severity/discussion/24532)
* [Bayesian Optimization XGBoost parameters](https://www.kaggle.com/tilii7/bayesian-optimization-xgboost-parameters)
* [XGboost + Bayesian Optimization](https://www.kaggle.com/tilii7/xgboost-bayesian-optimization/code)
* [SVR+sparse matrix+Bayesian optimization](https://www.kaggle.com/tilii7/svr-sparse-matrix-bayesian-optimization/)
* [Bayesian Optimization of a Technical Trading Algorithm with Zipline+SigOpt](https://blog.quantopian.com/bayesian-optimization-of-a-technical-trading-algorithm-with-ziplinesigopt-2/)
* [sklearn-gridsearchcv-replacement.ipynb](https://github.com/scikit-optimize/scikit-optimize/blob/master/examples/sklearn-gridsearchcv-replacement.ipynb)
* [BayesianOptimization/sklearn\_example.py](https://github.com/fmfn/BayesianOptimization/blob/master/examples/sklearn_example.py)
* [Hyper-parameter Optimization with keras?](https://github.com/fchollet/keras/issues/1591)
* [Keras + Hyperopt: A very simple wrapper for convenient hyperparameter optimization](https://github.com/maxpumperla/hyperas)
* [Effectively running thousands of experiments: Hyperopt with Sacred](https://gab41.lab41.org/effectively-running-thousands-of-experiments-hyperopt-with-sacred-dfa53b50f1ec)
* [Hyperopt tutorial for Optimizing Neural Networks' Hyperparameters - Vooban](https://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/)
* [hyperopt\_experiments.py](https://github.com/Lab41/pythia/blob/master/experiments/hyperopt_experiments.py)
* [Spearmint](https://github.com/HIPS/Spearmint)
* [GPyOpt](https://sheffieldml.github.io/GPyOpt/) a Python open-source library for Bayesian Optimization
* [Scikit-Optimize](https://scikit-optimize.github.io/)
* [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/neural-networks-3/#hyper)
* [Hyperparameters tuning with Polyaxon – Polyaxon – Medium](https://medium.com/polyaxon/hyperparameters-tuning-with-polyaxon-9403f8ea85be)
* [zygmuntz/hyperband: Tuning hyperparams fast with Hyperband](https://github.com/zygmuntz/hyperband)
* [jmcarpenter2/parfit: A package for parallelizing the fit and flexibly scoring of sklearn machine learning models, with visualization routines.](https://github.com/jmcarpenter2/parfit)
* [An Introductory Example of Bayesian Optimization in Python with Hyperopt](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0)
* [**A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning**](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)
* [The Hitchhiker’s Guide to Hyperparameter Tuning - Taboola Tech Blog](https://engineering.taboola.com/hitchhikers-guide-hyperparameter-tuning/)
* [Automated Machine Learning Hyperparameter Tuning in Python](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a)
* [The Hitchhiker’s Guide to Hyperparameter Tuning - Taboola Tech Blog](https://engineering.taboola.com/hitchhikers-guide-hyperparameter-tuning/)

## Automated

* [reiinakano/xcessiv: A web-based application for quick, scalable, and automated hyperparameter tuning and stacked ensembling in Python.](https://github.com/reiinakano/xcessiv)
* [ClimbsRocks/auto\_ml: Automated machine learning for analytics & production](https://github.com/ClimbsRocks/auto_ml)
* [Automatic model tuning with Sacred and Hyperopt.ipynb](https://github.com/gereleth/kaggle-telstra/blob/master/Automatic%20model%20tuning%20with%20Sacred%20and%20Hyperopt.ipynb)
* [Automated Machine Learning Hyperparameter Tuning in Python](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a)



## GridSearchCV

From [https://discussions.udacity.com/t/gridsearchcv-and-testingtraining-data/36107/4](https://discussions.udacity.com/t/gridsearchcv-and-testingtraining-data/36107/4)  
The error that you've provided is somewhat unrelated to the problems that you are having with GridSearchCV. Note the line upon which the error occurs:

```python
File "poi_id.py", line 244, in SVMAccuracyGridShuffle
feat_new=[feature_names[i]for i in pipe.named_steps['Select_Features'].get_support(indices=True)]
```

This is the third line of the code snippet posted. The error message notes that the pipeline needs to be fit to data first so that it can create the output attributes that you are trying to grab. If you are trying to get the names of the features selected by your parameter-selecting procedures, then you will need to do that after feature selection has been performed.In regards to the parameter selection and GridSearchCV questions, it's worth taking one more look at the overall process and the questions I posed at the end of my previous post. The image below might help give you another perspective that will help you understand the process visually:![](data:image/png;base64,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)  
GridSearchCV will be applied to the entire part of the data that you hold out for training \(and validation\). Once we have our algorithm and parameters selected, we no longer use GridSearchCV, and assess model performance on the part\(s\) of the data that we use for testing, fitting our selected algorithm on the remainder of the data.



Let's start with the code snippet:

```python
clf_cv = GridSearchCV(clf, ... )
clf_cv.fit(features_train, labels_train)
test_predict = clf_cv.predict(features_test)
f1_test = f1_score(labels_test, test_predict)
```

This corresponds with the following diagram:![](data:image/png;base64,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)  
Given training and test partitions \(upper half\), we first use cross-validation to find the best-fitting parameters using the training partition \(lower-left, demonstrated as 4-fold CV\), then assess the model's performance on the test partition \(lower-right\). This, taken on its own, does not seem to have any issues, but the question to think about at this point is how the \*\_train and \*\_test partitions were generated and how they are relevant to the data we have available.Have we performed a single split into training and test partitions? Do we have multiple partitions? If we have multiple splits, is there a way that we are assessing overall performance? How much information are we getting from performing a split into training and test partitions, considering how much data we have available? Is there a different way in which we should be performing our algorithm fitting and performance assessment?![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAoAAAAB6CAAAAAAKML2pAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH3wsQFhYkMEXsEAAAAAJiS0dEAP+Hj8y/AAACOElEQVR42u3dLU4DYRCA4Q9Z14OgkBVUcAJOxgkw7TF6BNIj1OFxKJKikLthf5L9ZuZ5LYYMTzPNwJZ2lzasGYEAFIASgKoLsGlCQ9PU1CECCGAvAK2Df6+NNvULGpoVgAACCCCAZgcggAACKAABBBBAAQgggAAKQAABBFAAAggggAIQQAABNDsAAQQQQAABBBDAFADlmRAPJQFYFaC0ySbu/H2CF0jyKQIIIIAAAtjx6D4Ou6crRkmnGADg6/n7/RGjpFOMsYI/9xglnWIIgF/PbxglnWIIgMcTRVmnGALgww9FWafoDFPnPeAdwJmje4Eo7RQBBBBAK9gKNjoAAQQQwM4G1xBMPsXW/ewQSj1FK9gKBjDstxdFoO9NGgNY84ms9R+M86TbzKfiAARwY4C3Uv0BvMxuCGC1OS6eIIAAAggggAACCCCAAAIIIIAAAggggAAGBCiHaAABrApQs/8YAUAAAbSCrWAAAQTQGSbMKWXpzwBAAAEEEEAAAQQQwMIAPROyzjMhAAIIYDyAWuszCAAEcOuPYZH/lgkggJLPhhGAVrCsYAABdIZJeEpZ/tsoAAEEEEAABSCAAAIoAAEEEEABCKBDtEO0AARQmV+zRiAArWArWAACWPkME/GU4gwDIIACEEAAARSAAAIIIIAAAggggA7RcogGEEBp7DVrBALQCraCBSCA8QHWOqU4wwAIoAAEEEAABSCAAAIIIIAAAgigQ7QcogEEUBp7zRqBABSAEoAq1y/CDo6dSHGt9AAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAxNS0xMS0xNlQyMjoyMjozNiswMDowMGe2m3UAAAAldEVYdGRhdGU6bW9kaWZ5ADIwMTUtMTEtMTZUMjI6MjI6MzYrMDA6MDAW6yPJAAAAAElFTkSuQmCC)  
Hopefully it's easier to see from here how the choice of what we do outside of the core snippet above has an impact on how the overall procedure looks. For more specific concerns regarding the way our model-fitting and assessment approach interacts with the data in the project, it'll probably be best to move the conversation back to the [more relevant topic](https://discussions.udacity.com/t/p5-testing-results-all-over-the-place/37850/).



According to Wiki, Cross-validation, sometimes called rotation estimation,[\[1\]](https://en.m.wikipedia.org/wiki/%23cite_note-1)[\[2\]](https://en.m.wikipedia.org/wiki/%23cite_note-Kohavi95-2)[\[3\]](https://en.m.wikipedia.org/wiki/%23cite_note-Devijver82-3) is a [model validation](https://en.m.wikipedia.org/wiki/Model_validation) technique for assessing how the results of a [statistical](https://en.m.wikipedia.org/wiki/Statistics) analysis will generalize to an independent data set.  
Suppose we have a [model](https://en.m.wikipedia.org/wiki/Statistical_model) with one or more unknown [parameters](https://en.m.wikipedia.org/wiki/Parameters), and a data set to which the model can be fit \(the training data set\). The fitting process [optimizes](https://en.m.wikipedia.org/wiki/Optimization_%28mathematics%29) the model parameters to make the model fit the training data as well as possible. If we then take an [independent](https://en.m.wikipedia.org/wiki/Independence_%28probability_theory%29) sample of validation data from the same [population](https://en.m.wikipedia.org/wiki/Statistical_population) as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data. This is called [overfitting](https://en.m.wikipedia.org/wiki/Overfitting), and is particularly likely to happen when the size of the training data set is small, or when the number of parameters in the model is large. Cross-validation is a way to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available.  
There are a couple of common types of cross-validation and the most common one is k-fold cross-validation[Edit](https://en.m.wikipedia.org/w/index.php?title=Cross-validation_%28statistics%29&action=edit&section=7)In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times \(the folds\), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling \(see below\) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used,[\[6\]](https://en.m.wikipedia.org/wiki/%23cite_note-McLachlan-6) but in general k remains an unfixed parameter.

