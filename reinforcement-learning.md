# Reinforcement Learning

* [Monte Carlo method in Python – Harder Choices](https://harderchoices.com/2018/04/04/monte-carlo-method-in-python/)
* [Monte Carlo Tree Search - beginners guide - Machine learning blog Machine learning blog](https://int8.io/monte-carlo-tree-search-beginners-guide/)
* [CS 294 Deep Reinforcement Learning, Fall 2017](http://rll.berkeley.edu/deeprlcourse/)
* [通過 Q-learning 深入理解強化學習 \| 機器之心](https://www.jiqizhixin.com/articles/2018-04-17-3)
* [Introduction to Various Reinforcement Learning Algorithms. Part I \(Q-Learning, SARSA, DQN, DDPG\)](https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287)
* [手把手：AlphaGo有啥了不起，我也能教你做一个（附Python代码） \| 机器之心](https://www.jiqizhixin.com/articles/2018-05-28-19)
* [詹皇比肩乔丹！如何防住他？这是AI给出的回答](https://zhuanlan.zhihu.com/p/36007263)
* [introdeeprl/introduction-deep-reinforcement.pdf at master · prichemond/introdeeprl](https://github.com/prichemond/introdeeprl/blob/master/introduction-deep-reinforcement.pdf)
* [Monte Carlo Tree Search - beginners guide - Machine learning blog Machine learning blog](https://int8.io/monte-carlo-tree-search-beginners-guide/)
* [Optimistic Q-Learning – Sequential Learning – Medium](https://medium.com/sequential-learning/optimistic-q-learning-b9304d079e11)
* [World Models](https://worldmodels.github.io/)
* [Pulkit-Khandelwal/Reinforcement-Learning-Notebooks: A collection of Reinforcement Learning algorithms from Sutton and Barto's book and other research papers implemented in Python.](https://github.com/Pulkit-Khandelwal/Reinforcement-Learning-Notebooks?mkt_tok=eyJpIjoiT1dVeVpHVTRaV1kwWkdGbSIsInQiOiI3VUdQU2dqbTNJdjJBQXp6NkhvekFqS3pRXC9BK1JjbE14cHc0VTdad3R5bzBjbmI2MER3aU5DWG9FdW1pblh3M2VJNmtxK0x2VnI3QTF0ZFpTRTAxbE80SGNLWngrVUMySWNVYVpwT1YrSmxtVGRiZ2N5cVRRQzVEZzRLOEh3ZDEifQ)
* [AlphaGoZero · Depth First Learning](http://www.depthfirstlearning.com/2018/AlphaGoZero)
* [deepmind/scalable\_agent: A TensorFlow implementation of Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures.](https://github.com/deepmind/scalable_agent)
* [Solving sparse-reward tasks with Curiosity – Unity Blog](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)
* [Building a Deep Neural Network to play FIFA 18 – Towards Data Science](https://towardsdatascience.com/building-a-deep-neural-network-to-play-fifa-18-dce54d45e675)
* [Neural scene representation and rendering \| DeepMind](https://deepmind.com/blog/neural-scene-representation-and-rendering/)
* [比TD、MC、MCTS指数级快，性能超越A3C、DDQN等模型，这篇RL算法论文在Reddit上火了 \| 机器之心](https://www.jiqizhixin.com/articles/2018-06-22-3)
* [Google AI Blog: Teaching Uncalibrated Robots to Visually Self-Adapt](https://ai.googleblog.com/2018/06/teaching-uncalibrated-robots-to_22.html)
* [OpenAI Five](https://blog.openai.com/openai-five/)
* [ctallec/world-models: Reimplementation of World-Models \(Ha and Schmidhuber 2018\) in pytorch](https://github.com/ctallec/world-models)
* [openai/retro at develop](https://github.com/openai/retro/tree/develop)
* [\[1805.07470\] Solving the Rubik's Cube Without Human Knowledge](https://arxiv.org/abs/1805.07470)
* [Reinforcement Learning from scratch – Insight Data](https://blog.insightdatascience.com/reinforcement-learning-from-scratch-819b65f074d8)
* [Pulkit-Khandelwal/Reinforcement-Learning-Notebooks: A collection of Reinforcement Learning algorithms from Sutton and Barto's book and other research papers implemented in Python.](https://github.com/Pulkit-Khandelwal/Reinforcement-Learning-Notebooks)
* [Practical Use Cases for Reinforcement Learning](https://www.reddit.com/r/MachineLearning/comments/8u6wo4/d_what_are_practical_use_cases_for_reinforcement/)
* [higgsfield/RL-Adventure-2: PyTorch0.4 implementation of: actor critic / proximal policy optimization / acer / ddpg / twin dueling ddpg / soft actor critic / generative adversarial imitation learning / hindsight experience replay](https://github.com/higgsfield/RL-Adventure-2)
* [MarI/O - Machine Learning for Video Games - YouTube](https://www.youtube.com/watch?v=qv6UVOQ0F44)
* [Capture the Flag: the emergence of complex cooperative agents \| DeepMind](https://deepmind.com/blog/capture-the-flag/)
* [Learning Montezuma's Revenge from a Single Demonstration](https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/)
* [ntaylorwss/pavlov: A modular, composable approach to reinforcement learning with a Keras backend and a functional data pipeline.](https://github.com/ntaylorwss/pavlov)
* [Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html)
* [Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://blog.openai.com/evolution-strategies/?__s=mnqizqqfa1zqmafmd1zr)
* [Train a Reinforcement Learning agent to play custom levels of Sonic the Hedgehog with Transfer Learning \| Felix Yu](https://flyyufelix.github.io/2018/06/11/sonic-rl.html)
* [Metacar: A reinforcement learning environment for self-driving cars in the browser.](https://www.metacar-project.com/)
* [Reinforcement Learning - Monte Carlo Methods · Ray](https://oneraynyday.github.io/ml/2018/05/24/Reinforcement-Learning-Monte-Carlo/)
* [Different Schools of Thought: How Silicon Valley Teaches Robots \| Synced](https://syncedreview.com/2018/07/10/different-schools-of-thought-how-silicon-valley-teaches-robots/)
* [Reinforcement learning’s foundational flaw](https://thegradient.pub/why-rl-is-flawed/)
* [Arthur Juliani on Twitter: "New blog post discussing DeepMind and OpenAI's work on solving the first level of Montezuma's Revenge using Deep RL, and why that isn't necessarily as exciting as it seems. Would love to hear other's thoughts. https://t.co/LpXOIRZa7B"](https://mobile.twitter.com/awjuliani/status/1017701709735489536)
* [An introduction to Reinforcement Learning – freeCodeCamp](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)
* [Explaining Reinforcement Learning: Active vs Passive](https://www.kdnuggets.com/2018/06/explaining-reinforcement-learning-active-passive.html)
* [A machine has figured out Rubik’s Cube all by itself - MIT Technology Review](https://www.technologyreview.com/s/611281/a-machine-has-figured-out-rubiks-cube-all-by-itself/)
* [An Autonomous Car Learned how to Drive itself in 20 minutes using Reinforcement Learning](https://www.analyticsvidhya.com/blog/2018/07/autonomous-car-learnt-drive-itself-20-minutes-using-reinforcement-learning/)
* [非得從零開始學習？扒一扒強化學習的致命缺陷 \| 機器之心](https://www.jiqizhixin.com/articles/2018-07-13-5)
* [Evolutionary algorithm outperforms deep-learning machines at video games - MIT Technology Review](https://www.technologyreview.com/s/611568/evolutionary-algorithm-outperforms-deep-learning-machines-at-video-games/?goal=0_997ed6f472-7b40bc8a04-157972765)
* [2018\_08\_02\_Amazon-SF-ML-Meetup-Abbeel-final.pdf](https://www.dropbox.com/s/bwksh3zu8ae37ly/2018_08_02_Amazon-SF-ML-Meetup-Abbeel-final.pdf?dl=0)
* [Prefrontal cortex as a meta-reinforcement learning system \| DeepMind](https://deepmind.com/blog/prefrontal-cortex-meta-reinforcement-learning-system/?mkt_tok=eyJpIjoiWm1NNFpEUXpNek00TVRkbSIsInQiOiI1ZWEwejVJeUs4SVFhMFJ4b0F5NkZNRlpsTWlqRVdqcWFta001Mkd0YTQzbHY2Qnl0aDlaRkJEM1FEbGN6Ykx6ejFFNW1UVFZDME5jZmd1VEUyYXBBeFQ3VzhCZkRmVDc3dzF3OE1tUUYyYzI4bVo1Vmg5ZXVaUzFFbkphNEwrYiJ9)
* [On “solving” Montezuma’s Revenge – Arthur Juliani – Medium](https://medium.com/@awjuliani/on-solving-montezumas-revenge-2146d83f0bc3)
* [\[1806.05695\] Evolving simple programs for playing Atari games](https://arxiv.org/abs/1806.05695)
* [How to fix reinforcement learning](https://thegradient.pub/how-to-fix-rl/)
* [Deep Reinforcement Learning: Playing CartPole through Asynchronous Advantage Actor Critic \(A3C\)…](https://medium.com/tensorflow/deep-reinforcement-learning-playing-cartpole-through-asynchronous-advantage-actor-critic-a3c-7eab2eea5296?linkId=54979587)
* [DOTA 5v5 AI 的亮点不是如何「学」的，而是如何「教」的 \| 雷锋网](https://www.leiphone.com/news/201808/4vLWPWlmh382StPY.html)
* [\[1701.07274\] Deep Reinforcement Learning: An Overview](https://arxiv.org/abs/1701.07274)
* [Guest Post \(Part I\): Demystifying Deep Reinforcement Learning - Intel AI](https://ai.intel.com/demystifying-deep-reinforcement-learning/)
* [A simple guide to Reinforcement Learning by thoughtram](https://blog.thoughtram.io/machine-learning/2018/02/28/a-simple-guide-to-reinforcement-learning.html)
* [AlphaGo, in context – Andrej Karpathy – Medium](https://medium.com/@karpathy/alphago-in-context-c47718cb95a5)
* [Reinforcement learning explained - O'Reilly Media](https://www.oreilly.com/ideas/reinforcement-learning-explained)
* [Keras plays catch - a single file Reinforcement Learning example](https://gist.github.com/EderSantana/c7222daa328f0e885093)
* [强化学习路在何方？](https://zhuanlan.zhihu.com/p/39999667)
* [Introduction to Various Reinforcement Learning Algorithms. Part II \(TRPO, PPO\)](https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9)
* [kitao/pyxel: A retro game engine for Python](https://github.com/kitao/pyxel) 
* [andri27-ts/60\_Days\_RL\_Challenge: Learn Deep Reinforcement Learning in depth in 60 days](https://github.com/andri27-ts/60_Days_RL_Challenge)
* [tristandeleu/pytorch-maml-rl: Reinforcement Learning with Model-Agnostic Meta-Learning in Pytorch](https://github.com/tristandeleu/pytorch-maml-rl)
* [Microsoft/malmo: Project Malmo is a platform for Artificial Intelligence experimentation and research built on top of Minecraft. We aim to inspire a new generation of research into challenging new problems presented by this unique environment. --- For installation instructions, scroll down to _Getting Started_ below, or visit the project page for more information:](https://github.com/Microsoft/malmo)

**RL and teamwork** OpenAI has announced that they are [starting to beat human players in 5v5 Dota 2 matches](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqCrSX7tlXWXWxm65OpVW7LC-2FTI08BlQBfqlZ2XbPhEW2_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2BblzrSTRMZac3OLz4Z1n7pVKk6lj7KBJjyeXZUXaD9mi-2BbEB12vQusPFQllSl-2B-2BIMQSqIqaTvtLATa94anofHWWmfbMh8QpxOH1vhbcwHV-2BYjoOoOTD5pws2-2BMRPvZqRUiEBKG53Cgqo6zOO6W-2FY7s-2B2M). Their approach uses a massively scaled-up version of PPO and a large LSTM to represent each model, two approaches that were thought to struggle with modelling long-term dependencies. Models are trained via self-play.

Remarkably, the models learn to act as a team by putting more weight on the teammates’ reward functions as training progresses. [Stephen Merity](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqPnygrbYtlNI9MNpRQnM3eEwcybgK6nDSx3mvEe5zOhfJovnM1-2B8CSdATybbzkOLrQ-3D-3D_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2Bblw19eX-2BLGs9Va1zTQFTtPNo-2FWWeuoS5FfjtJwTG0QWw0fBEV70kd-2F8C-2FaWaDd2Z-2FBLtJbE6yDbPQK-2B7RquJ0HHnDoEo4FL4lXP3TXnXvJuq3CyT-2FvbiutOEAJKslWzCl94UM3s1dLq9dbhhVVZ072j1), [Catherine Olsson](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqE4ExjTPinZlcSzjYAkduHX5lqN-2BcRdDrshTBl7bY-2FtAO5t7ISXfCY7ixrVpiTXdqxJGr6Tjp3M-2BYbd21H51320-3D_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2BblxdYekTAMkD1s1YHlICXx0ZYPGlFVcpXV-2FG4kyQWSzuufGteu-2B3SKr41kD1qy7150V-2FmAG2bxQTL8FXs8TUnD9wVDHomYzf1e8QsSRfuMS1Awn27uqvW88aH-2FnbYwpWYPL9d2bvQGT4yRYYLa0hffOA), and [Alex Irpan](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqPUH0mMmD-2Fg-2BGvvozKNpEvPE3Qae4G5t0Oq5giaObOiPtYaQ4xwdxlH6hYAnEeZewg-3D-3D_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2BblyJtOfWi-2FlUakxj3hitbf6teSKHTAlSUJqbRX0IP59XJFhNZvzZIDADLzX4GXV5XQk-2FgNdqPgt-2FDvDV6J204Vm1GGBDQZgjtJk7aWX6BM0EWXfT6cQWvdoVhZBTQF51mlkl5rLwedjjoauZYHxtP96p), among others, have commented on why **teamwork** is important for AIs to learn. [Sam Altman](http://click.revue.email/wf/click?upn=Ep-2BprbLTAIJfGzDYMtK2QinS-2FfuG7Xj1W1MOZ0MOhJCrXlO3hd0pThHU29WJm1qugjxctzhrx6EcaKjApFp6p7VtxsnK-2Bcgwp99ojaLWirY-3D_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2BblxfiFSpWAjXK-2FxP3xM84ZTSlSgk55K2Rwo4lLDIhtJekaBgWgbwkvyzCSUiEJhy1-2FGK0H6lY5mL4I-2ByAd7jW6png3s5qEsT-2FlMza3HTKSIJW3xzUvINnA02V58Fc8o-2FC-2F3MbRpY1a1b-2Bok0ylYmGYlV), president of YC and OpenAI sponsor, is also bullish about the potential of RL.

DeepMind trained agents for a [Capture The Flag game ](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqMG5DohSxq5fNtMdQwuoDfXIes6HA17blOqitPqR8YdQGI95jar49aTwXI4appaWog-3D-3D_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2Bblxx56MBCf2q1Zp6z5ybGNzLgMPfSCmulqhVcXoO-2FQ1HVqnzs52UMH9MEJvYm-2FTEAph2SuECBS3gxrgVY93VC-2Bqv4pdw9S-2BNi8WbNy-2BPz3MSQjpUfgv5YfnAdsZ1x0y6EpGQ0hJTmYbS-2BfPN9v-2BCnHDZ)using their [**population-based training**](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqLcUs6pDD9HhxibCLHxpwzkLZi7hVmro1WwKSwtPtKuB_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2Bblw961N8YtErYZfxuN-2BYS9hLqAQ2Mlkzxrk4hAp3Rq1XMHJ1og-2F7H22itvOwFoqNOagXjXyfIB2EJtZfvqugPShuvJOR7uQfw4vmF7zT2G0cfgZrxpaMVlyIt3FJAr2bunvJoDVjJY348pE7-2FupR5ky2) method, which jointly trains a populationof agents rather than a single one. Without explicitly encouraging them, teammates also learn to cooperate.

Finally, OpenAI [trained an agent to state-of-the-art performance on Montezuma’s Revenge](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqCrSX7tlXWXWxm65OpVW7LClLbBIsFg2wEcWvVWD7ML-2FGKG-2FOoHsATWPoTrOUFqzdQii0qbQfulMS8ntRv4aB-2Bd8fJUarLnk6fFrLQJPc822_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2BblyLxq8v6nYyOh5SYfB18kiI85DtppB9uyxlhDIEuYeYPtP15W9mPAZeUgFR9XFU6GK8pjyoRBMeSmrOOLvRdAsbVy25wdMEKyylffl-2Fm9PWLCOaqHsK3SoVLRqPi-2FOB6UfLfYYc2ZWV7Pa7Yge9ht2E), a notoriously difficult game, **using a single demonstration**. They use states in the demonstration to construct a curriculum that becomes progressively harder: At the first subtask, the model starts from the state just before the reward. After learning to achieve the reward, the model starts from a state that is slightly earlier, and so on. Different to imitation learning, the model does not learn to copy the exact behaviour but can divert from the demonstration if it is beneficial.

Another way to learn from demonstrations is **inverse reinforcement learning**, which aims to learn the reward function from observed behaviour. TheGradient has a [great overview article about this method](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqK9hZhrdQMv7VUrjNwcq9UOaX-2F6hvZLu4Kb2yyzfgJHzGc3ykwNXDUIkAZAnDWTWfWQ81FxYTT0F6iuLmSFPLHsSMtmL30hjb-2BkaiZ7f72d0_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2BblyiY8lmuPZiq3unjQlvA19VQtrOmAbMjf7XhZccy5-2BwgLTpvvrmqph4acnh6FMFR4bWgChL5unLn-2BUZm41CIJkOT4wr0RWFY4GA9SFwsHbTMLOusjLC-2F7HoEMIKkq8MSjUoh0xdCY16m3Vyhhkhn1q4). Check out these articles if you want to read about the [fundamental flaw of RL](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqK9hZhrdQMv7VUrjNwcq9UP7ZeZSg-2BynWISoPNNZgsEBuOe7-2B4bin2yE3j11tEzcPQ-3D-3D_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2BblxoDoozhWalUJzTROt7ZuL7U39aPt7YmRYyR1zAaUBQJwIR-2F7GirUXSB5UduL-2BJovAhd7s8xesk-2BC82ff4ZwOFIgAAVJqnAZ9cb1KP66NJngh0gZt4jzVF21UvnRWYkvk0nDis5DuLaVOVYTzDWHlvg) and [how to fix it](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqK9hZhrdQMv7VUrjNwcq9UMhp8WRscY4z6uBQXL-2F-2BBK4_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2Bblw2EHPN2PzfJyJHMzsnieQ3xLVMzVs71dfqZDGNVSh0zuUPzBNYTPYoLe9SYZrdzj0-2BsExL9NnNjkM7XhpgKX6M8ah4FrmCyhfmoDgf7zezTrJpAGu5P2YnDgKG7-2FHXfW1kia0Nl6LoYUVkrIEu1VCS).

For **humans and AI** working together, have a look at [this article](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqJxeGS2SAJAypryIhlW1dfDC682X1xXPDMe4TU7A3V8XoT0cWfmfRsMRqO-2BbeB7JS69bk0fHfbzjJG878itOusrkkPZMPokYFnobr37iC-2FKl_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2BblzKvDxh-2Bk-2BzGtQOGPhj2nGvstg3PR5RnJ-2BCsucxmPmRmqtGIdjLUIh-2FvJspUYn7ueHTubhKupplZDRz-2FMNJNWiaFlijeKyFAj3itdC72DPLseeti7fASRk4VTvqfeIxiZjSD32ZM7ACzu8UNUj7P-2Fx-2B) where Harvard Business Review highlights the results of a survey of 1,500 companies. On the topic of human and AI collaborating, remember **IBM’s Project Debater** discussed in the [last newsletter](http://click.revue.email/wf/click?upn=pp5IBAXuSEQLAhgzr04TJhWumODQuu025Hr6TcL8iDBTCefhrfYIjtrZwO8SfDMUpJ-2FEHS2WgSEK6hgKoEDIRG7YBH-2FhgUvkYsDXfB09kzpz-2FJO1tprODNx0OtJuqz2QGHU4ihWLMNkoxTnqCspJNdUntZ4wL2pnZUUeMN6103qdiHC0nogYghsbnvrtNFTFQksVPMuzSu7go8LQxoDzrP0093AFgcO0RFmFWqvhGnlPhNWwpIS9wA6Yfd0sQSp4mtyBBMChgZTOipXEFXL3Vh8WlnTjL2Az7aTymuAClAc-3D_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2BblxPTHFgCJLlcUCcSsh-2FLzVR9KcmQNliyRYuOQu5Sgkqg1FS-2FcWGS-2BbYSTTVYRVzxfRmu1iDgOOpPCPSAVZGx6v1CCLa54ThQQfDou8Ul8qPNUlTLtUHPsMY1D-2Fb4qJcwIBnMu2ZcJpwalbkZg0NMQRF)? IBM presented a [demo paper at NAACL 2018](http://click.revue.email/wf/click?upn=WRRLVfAKM9x85h3ZSET3wuKwF4Gc-2FjX-2FRNs7nkiqoEm-2FI44p0H-2B7COvpURK-2BX5Dg_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2Bblxy0Fd4zVK-2FAcqa-2BC9F6RYQ5SPNo2doomzdqWJOtOEjmlIV35r6-2FujZI-2FjOTGtwEUTeYki6jqThrYOmFWinkxsYrI0owBO-2BLdLt36Dhz-2FTCD6S1BFpRprX2QFzddhWLR4ql-2Biu5dcojKNlYH1EIB6x4) about a component of it, which is further discussed in this [blog post](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqMWrkJNjaej7FC0m1Hdirvz2vmX9dlfLj7gaUPt0Qxn7EokNP-2F-2Bn0vTxrPag7Z3CWLXgu4tk5MPc7GtehSwPTxo-3D_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2BblwDKcqFRG-2B6Jp4ILF86j720Hu-2Bxb6Id9z2ocqjIiUK1NTe79vBm9H4HJMItcvMZ8EaCdgez4Ht3oxyhTiWnKVtMhXVISYXD5wCc7ihaZyEgE6o-2BmtPZ7RYTNuf52OqusoC38SzJIa9d5XuP-2FYZHIz-2Bc). The discussed model is trained to analyze and summarize a discussion, but still seems far away from being an active participant in a debate.

For **RL on robots**, models similarly perform better if they’re trained on more data. Many algorithms, however, require data to be on-policy, i.e. the data has to be generated with the current policy. Google researchers designed a [new off-policy algorithm ](http://click.revue.email/wf/click?upn=uoSvyDIrlYZBTfKX9q-2FuqDx10WExyAD-2FOedorI1vj2UnSp-2FT6DWaEk0k7mTWN6akL8d3iNsz1wijnI8TFQQax4FKfeG0nOkh7f0WvLCA94Chs2S0gmF3dQkCYUNIJLIb_cjJwF5rVq6bcAJDW2bYhckKcDVGtuI9UhBbWA-2B2uuED-2F9IWETQaRREn2Tf47sHTGZXO9ja9OTZm9PlSqBgFXzrz4-2B7mi5GxSleqUk2Np89Q6DluxDFEZd8AozLMBBcrKilelEDTcPDXgbMmuxJfSOG6Nel2oeYrWBj7mHpT-2BblztBot4vGm-2FeziUKLZyV5HnphmjSBUeVieEmawaRXNaC-2B-2BvluZMHQjl-2FyOVtlTNcgp60oLuQXwA0y1H-2BicgpAfWd0m3Ys0ucFd4Pa6x9tQip-2BIeYlI8-2Fugfq8s-2BvlQPeVOqUydOVUkFy7WoSUJrTdfC)that can benefit from large amounts of data acquired in the past.



{% embed data="{\"url\":\"https://mobile.twitter.com/dennybritz/status/1019577351900946435\",\"type\":\"rich\",\"title\":\"Denny Britz on Twitter\",\"description\":\"I wonder how @OpenAI came up with their model architecture for Dota 2. Did they use some kind of surrogate supervised task to optimize the architecture? Or was is it common sense + domain knowledge of how to best represent the features? @jackclarkSF @gdb ? pic.twitter.com/UfUmb3XNPm— Denny Britz \(@dennybritz\) July 18, 2018\\n\\n\",\"icon\":{\"type\":\"icon\",\"url\":\"https://abs.twimg.com/icons/apple-touch-icon-192x192.png\",\"width\":192,\"height\":192,\"aspectRatio\":1},\"thumbnail\":{\"type\":\"thumbnail\",\"url\":\"https://pbs.twimg.com/media/DiZEMK1WkAEF440.jpg:large\",\"width\":1801,\"height\":955,\"aspectRatio\":0.5302609661299278},\"embed\":{\"type\":\"app\",\"html\":\"<blockquote class=\\\"twitter-tweet\\\" data-dnt=\\\"true\\\" align=\\\"center\\\"><p lang=\\\"en\\\" dir=\\\"ltr\\\">I wonder how <a href=\\\"https://twitter.com/OpenAI?ref\_src=twsrc%5Etfw\\\">@OpenAI</a> came up with their model architecture for Dota 2. Did they use some kind of surrogate supervised task to optimize the architecture? Or was is it common sense + domain knowledge of how to best represent the features? <a href=\\\"https://twitter.com/jackclarkSF?ref\_src=twsrc%5Etfw\\\">@jackclarkSF</a> <a href=\\\"https://twitter.com/gdb?ref\_src=twsrc%5Etfw\\\">@gdb</a> ? <a href=\\\"https://t.co/UfUmb3XNPm\\\">pic.twitter.com/UfUmb3XNPm</a></p>&mdash; Denny Britz \(@dennybritz\) <a href=\\\"https://twitter.com/dennybritz/status/1019577351900946435?ref\_src=twsrc%5Etfw\\\">July 18, 2018</a></blockquote>\\n<script async src=\\\"https://platform.twitter.com/widgets.js\\\" charset=\\\"utf-8\\\"></script>\\n\",\"maxWidth\":550,\"aspectRatio\":null}}" %}

## Platform

* [Notes from the first Ray meetup - O'Reilly Media](https://www.oreilly.com/ideas/notes-from-the-first-ray-meetup?mkt_tok=eyJpIjoiWXpKaU1XRmlNVE14WXpZeCIsInQiOiJ3cjM1cE81Qnh6cDNmYldrVFRhXC9kMkNVN2IyVFVWYzRKYlYzMTJVRXBubXZmMzdWQWRBclRRemo4Z25wVUhRZnh2aWhPbWVGR0pJYllZa1RcL0poT1Ixb0hxSUpQcnFYVlk3Mm5vbmtPS2VmcWk4NmJYZ3BZS05Od2lWUzNQRUZ0In0)

