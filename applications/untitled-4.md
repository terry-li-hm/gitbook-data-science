# NLP

[Visualizing A Neural Machine Translation Model \(Mechanics of Seq2seq Models With Attention\) â€“ Jay Alammar â€“ Visualizing machine learning one concept at a time](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

[Intuitive Understanding of Word Embeddings: Count Vectors to Word2Vec](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)

[How to get started in NLP â€“ Towards Data Science](https://towardsdatascience.com/how-to-get-started-in-nlp-6a62aa4eaeff)

[How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)

[Data Analysis & XGBoost Starter \(0.35460 LB\) \| Kaggle](https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb)

[Bag of Words Meets Bags of Popcorn \| Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial#description)

[Working With Text Data â€” scikit-learn 0.19.1 documentation](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)

[sloria/TextBlob: Simple, Pythonic, text processing--Sentiment analysis, part-of-speech tagging, noun phrase extraction, translation, and more.](https://github.com/sloria/textblob)

[Getting Started with spaCy for Natural Language Processing](https://www.kdnuggets.com/2018/05/getting-started-spacy-natural-language-processing.html)

[How I lost a silver medal in Kaggleâ€™s Mercari Price Suggestion Challenge using CNNs and Tensorflow](https://medium.com/unstructured/how-i-lost-a-silver-medal-in-kaggles-mercari-price-suggestion-challenge-using-cnns-and-tensorflow-4013660fcded)

[Understanding Feature Engineering \(Part 4\) â€” Deep Learning Methods for Text Data](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)

[fastText/pretrained-vectors.md at master Â· facebookresearch/fastText](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)

[Kyubyong/nlp\_tasks: Natural Language Processing Tasks and References](https://github.com/Kyubyong/nlp_tasks)

[xiamx/awesome-sentiment-analysis: ðŸ˜€ðŸ˜„ðŸ˜‚ðŸ˜­ A curated list of Sentiment Analysis methods, implementations and misc. ðŸ˜¥ðŸ˜ŸðŸ˜±ðŸ˜¤](https://github.com/xiamx/awesome-sentiment-analysis)

[The Essential NLP Guide for data scientists \(codes for top 10 NLP tasks\)](https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/)

[How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)

[What is TF-IDF? The 10 minute guide](http://michaelerasm.us/post/tf-idf-in-10-minutes/)

According to [Wikipedia](https://en.wikipedia.org/wiki/Word_embedding):

> Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing \(NLP\) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much lower dimension.

Further readings:[https://www.quora.com/What-does-the-word-embedding-mean-in-the-context-of-Machine-Learning/answer/Julien-Despois](https://www.quora.com/What-does-the-word-embedding-mean-in-the-context-of-Machine-Learning/answer/Julien-Despois)[https://www.tensorflow.org/tutorials/word2vec\#motivation\_why\_learn\_word\_embeddings](https://www.tensorflow.org/tutorials/word2vec#motivation_why_learn_word_embeddings)[https://www.zhihu.com/question/32275069](https://www.zhihu.com/question/32275069)

[Awesome-Chinese-NLP: A curated list of resources for Chinese NLP ä¸­æ–‡è‡ªç„¶èªžè¨€è™•ç†ç›¸é—œè³‡æ–™](https://github.com/crownpku/Awesome-Chinese-NLP)

[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)



* [How can I tokenize a sentence with Python?](https://www.oreilly.com/learning/how-can-i-tokenize-a-sentence-with-python)
* [è‡ªç„¶èªžè¨€è™•ç†å¾žå…¥é–€åˆ°é€²éšŽè³‡ä»£ç¢¼è³‡æºåº«å½™ç¸½ï¼ˆéš¨æ™‚æ›´æ–°ï¼‰](https://zhuanlan.zhihu.com/p/28616862)
* [è‰¾ä¼¦AIç ”ç©¶é™¢å‘å¸ƒAllenNLPï¼šåŸºäºŽPyTorchçš„NLPå·¥å…·åŒ…](https://www.jiqizhixin.com/articles/2017-09-09-5)
* [Deep Learning for NLP Best Practices](http://ruder.io/deep-learning-nlp-best-practices/)
* [Topic Modelling Financial News with Natural Language Processing](http://mattmurray.net/topic-modelling-financial-news-with-natural-language-processing/)
* [Best Practices for Document Classification with Deep Learning](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/)
* [Natural Language Processing in Artificial Intelligence is almost human-level accurate. Worse yet, it gets smart!](https://sigmoidal.io/boosting-your-solutions-with-nlp/)
* [Word vectors for non-NLP data and research people](https://medium.com/towards-data-science/word-vectors-for-non-nlp-data-and-research-people-8d689c692353)
* [Deep Learning for NLP Best Practices](http://ruder.io/deep-learning-nlp-best-practices/)
* [åˆå­¦è€…æŒ‡å—ï¼šç¥žç»ç½‘ç»œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨](https://www.jiqizhixin.com/articles/2017-09-17-7)
* [A gentle introduction to Doc2Vec](https://medium.com/towards-data-science/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)
* [Word embeddings in 2017: Trends and future directions](http://ruder.io/word-embeddings-2017)
* [Word Embedding in Deep Learning](https://analyticsdefined.com/word-embedding-in-deep-learning/)
* [Using pre-trained word embeddings in a Keras model](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)
* [Deep Learning for Natural Language Processing: 2016-2017](https://github.com/oxford-cs-deepnlp-2017/lectures)
* [åŸºäºŽSpark /Tensorflowä½¿ç”¨CNNå¤„ç†NLPçš„å°è¯•](http://www.jianshu.com/p/1afda7000d8e)
* [Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)
* [Embedding projector - visualization of high-dimensional data](http://projector.tensorflow.org/)
* [Pytorch implementations of various Deep NLP models in cs-224n\(Stanford Univ\)](https://github.com/DSKSD/DeepNLP-models-Pytorch)
* [Stop Using word2vec](http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/)
* [è®©æœºå™¨åƒäººä¸€æ ·äº¤æµï¼šæ–¯å¦ç¦æŽçºªä¸ºåšå£«æ¯•ä¸šè®ºæ–‡](https://www.jiqizhixin.com/articles/2017-11-14)
* [Gentle Introduction to Statistical Language Modeling and Neural Language Models](https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/)
* [Dan Jurafsky & Chris Manning: Natural Language Processing \(great intro video series\)](https://www.youtube.com/watch?v=nfoudtpBV68&list=PL6397E4B26D00A269)



[Data Science 101 \(Getting started in NLP\): Tokenization tutorial \| No Free Hunch](http://blog.kaggle.com/2017/08/25/data-science-101-getting-started-in-nlp-tokenization-tutorial/)

[Vector Representations of Words  \|  TensorFlow](https://www.tensorflow.org/tutorials/word2vec) \(highly recommneded by Jeremey\)

[NLP â€” Building a Question Answering model â€“ Towards Data Science](https://towardsdatascience.com/nlp-building-a-question-answering-model-ed0529a68c54)

[Entity extraction using Deep Learning based on Guillaume Genthial work on NER](https://medium.com/intro-to-artificial-intelligence/entity-extraction-using-deep-learning-8014acac6bb8)

[Text Classification using machine learning â€“ Nitin Panwar â€“ Technical Lead \(Data Science\), Naukri.com](http://nitin-panwar.github.io/Text-Classification-using-machine-learning/)

[Unsupervised sentence representation with deep learning](https://blog.myyellowroad.com/unsupervised-sentence-representation-with-deep-learning-104b90079a93)

[**How to solve 90% of NLP problems: a step-by-step guide**](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)** \(11.3k clap!\)**

[Building a FAQ Chatbot in Python â€“ The Future of Information Searching](https://www.analyticsvidhya.com/blog/2018/01/faq-chatbots-the-future-of-information-searching/)

[Sentiment analysis on Trump's tweets using Python](https://dev.to/rodolfoferro/sentiment-analysis-on-trumpss-tweets-using-python-?)

[Improving Airbnb Yield Prediction with Text Mining â€“ Towards Data Science](https://towardsdatascience.com/improving-airbnb-yield-prediction-with-text-mining-9472c0181731)

[Machine Learning with Text in scikit-learn \(PyCon 2016\) - YouTube](https://www.youtube.com/watch?v=ZiKMIuYidY0&list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&index=10)

### Sentiment Analysis

[Perform sentiment analysis with LSTMs, using TensorFlow - O'Reilly Media](https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow)

[Data Science 101: Sentiment Analysis in R Tutorial \| No Free Hunch](http://blog.kaggle.com/2017/10/05/data-science-101-sentiment-analysis-in-r-tutorial/)

### Text Classification

[A Comprehensive Guide to Understand and Implement Text Classification in Python](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)

[Big Picture Machine Learning: Classifying Text with Neural Networks and TensorFlow](https://medium.freecodecamp.org/big-picture-machine-learning-classifying-text-with-neural-networks-and-tensorflow-d94036ac2274)



[Analyzing tf-idf results in scikit-learn - datawerk](https://buhrmann.github.io/tfidf-analysis.html)



Tf-idf stands for_ term frequency-inverse document frequency_

> Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency \(TF\), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency \(IDF\), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.
>
> * **TF: Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length \(aka. the total number of terms in the document\) as a way of normalization:   TF\(t\) = \(Number of times term t appears in a document\) / \(Total number of terms in the document\).
> * **IDF: Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as "is", "of", and "that", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:   IDF\(t\) = log\_e\(Total number of documents / Number of documents with term t in it\).
>
> See below for a simple example.
>
> **Example:**
>
> Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency \(i.e., tf\) for cat is then \(3 / 100\) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency \(i.e., idf\) is calculated as log\(10,000,000 / 1,000\) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 \* 4 = 0.12.

